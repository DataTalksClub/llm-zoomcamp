# Data Preparation in RAG

## 1. Ingest

<img src="https://github.com/user-attachments/assets/9cc07237-add9-462e-9272-6e52eb918c9a">

In this section, we cover the ingestion of documents from a single data source.

- https://youtu.be/jTWFh8ocDDY
- https://youtu.be/eg7xPhGWCcU

## 2. Chunk

Once data is ingested, we break it into manageable chunks.
This section explains the importance of chunking data and various techniques.

- https://youtu.be/aZkdusiBr10

## 3. Tokenization

Tokenization is a crucial step in text processing and preparing the data for effective retrieval.

- https://youtu.be/SpoepeljNGc

## 4. Embed

Embedding data translates text into numerical vectors that can be processed by models.

- https://youtu.be/BmlDmGMnrEA

## 5. Export

After processing, data needs to be exported for storage so that it can be retrieved for better contextualization of user queries.

- https://youtu.be/nhdG09vZqtc

## 6. Test Vector Search Query

After exporting the chunks and embeddings, we can test the search query to retrieve relevant documents on sample queries.

- https://youtu.be/BDgzv5nDt5g
- https://youtu.be/2A7h4dWV8xA

## 7. Trigger Daily Runs

Automation is key to maintaining and updating your system.
This section demonstrates how to schedule and trigger daily runs for your data pipelines, ensuring up-to-date and consistent data processing.

- https://youtu.be/7JyWw1F50CE

## Getting started

1. Follow these Docker instructions: https://docs.mage.ai/getting-started/setup#docker-compose-template
1. Use the docker image `mageai/mageai:llm`

## Code

- https://github.com/mage-ai/rag-project

## Playlist

[All videos](https://www.youtube.com/playlist?list=PL_ItKjYd0Dsg86be-K5GqMbA4VLBJT5Pc)
