import os
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

from documents_database import (
    setup_es_client_and_index,
    dump_doc_embeddings_to_db,
    extend_ground_truth_dataset,
    elastic_search_fields
)

# from utils.llm_utils import ask_llm, build_prompt

ES_INDEX_NAME = "course_questions"

OPEN_API_KEY = os.environ['OPENAI_API_KEY']


def compute_cosine_similarity(vectors, index_name):
    for vector in vectors:
        cosine_similarity_value = cosine_similarity(
            [vector["_source"]["text_vector"]],
            [vector["_source"]["llm_answer_vector"]])[0][0]
        es_client.update(
            index=index_name,
            id=vector["_id"],
            doc={
                "cosine_similarity_text_llm_answer": cosine_similarity_value
            }
        )


def compute_metrics(ground_truth_data: pd.DataFrame):
    # (2) LLM-as-a-judge: toxicity https://docs.evidentlyai.com/user-guide/customization/huggingface_descriptor
    # (3) LLM-as-a-judge: ask anything, i.e. if answer makes sense https://docs.evidentlyai.com/user-guide/customization/llm_as_a_judge
    # need to find out if this can only be applied on one column or on two as well to compare expected answer vs. actual answer
    prompt = """
        Please assess if the answer generated by an LLM LLM_ANSWER is suitable to answer a given question: QUESTION

        Use the following categories to judge the quality:
        good if the llm_answer is suitable to answer the question
        ok if the llm_answer is partly suitable to answer the question
        bad if the llm_answer is not suitable to answer the question
        Return only one category.
    """
    report = Report(metrics=[
        TextEvals(column_name="llm_answer", descriptors=[
            OpenAIPrompting(
                prompt=prompt,
                prompt_replace_string="LLM_ANSWER",
                context_replace_string="QUESTION",
                context_column="question",
                model="gpt-3.5-turbo-instruct",
                feature_type="cat",
                display_name="Assess llm answer towards initial question"
            )
        ])])
    report.run(reference_data=None, current_data=ground_truth_data_sample,
               column_mapping=column_mapping)
    report_df = report.as_dataframe()
    print(report_df.info())
    print(report_df)

    # report_df.to_csv("metrics_report.csv")


if __name__ == "__main__":
    es_client = setup_es_client_and_index(index_name=ES_INDEX_NAME)
    dump_doc_embeddings_to_db(es_client=es_client, index_name=ES_INDEX_NAME)
    extend_ground_truth_dataset(es_client=es_client, index_name=ES_INDEX_NAME)
    result = elastic_search_fields(es_client=es_client, index_name=ES_INDEX_NAME, search_query={
        "query": {
            "bool": {
                "must": [
                    {"exists": {"field": "text_vector"}},
                    {"exists": {"field": "llm_answer_vector"}}
                ]
            }
        }
    })

    compute_cosine_similarity(result, index_name=ES_INDEX_NAME)

    # compute_metrics(pd.read_csv("ground-truth-data.csv",
    #                 usecols=["question", "contexts", "text", "llm_answer"]))
