import os
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

from documents_database import (
    setup_es_client_and_index,
    dump_doc_embeddings_to_db,
    extend_ground_truth_dataset,
    elastic_search_fields
)
# from utils.postgres import (
#     POSTGRES_DB_PARAMS,
#     create_metrics_db,
#     create_metrics_table,
#     store_metrics
# )
# from utils.llm_utils import ask_llm, build_prompt


# POSTGRES_DB_PARAMS['host'] = os.getenv('POSTGRES_HOST', 'localhost')
# POSTGRES_DB_PARAMS['dbname'] = 'metrics_db'

# Initialize database and table


ES_INDEX_NAME = "course_questions"

OPEN_API_KEY = os.environ['OPENAI_API_KEY']


def compute_cosine_similarity(vectors):
    text_vectors = []
    llm_answer_vectors = []

    for vector in vectors:
        text_vectors.append(vector["text_vector"])
        llm_answer_vectors.append(vector["llm_answer_vector"])

    text_vectors = np.array(text_vectors)
    llm_answer_vectors = np.array(llm_answer_vectors)

    cosine_similarities = cosine_similarity(text_vectors, llm_answer_vectors)

    print(f"Cosine Similarity: {cosine_similarities}")


def compute_metrics(ground_truth_data: pd.DataFrame):
    # (2) LLM-as-a-judge: toxicity https://docs.evidentlyai.com/user-guide/customization/huggingface_descriptor
    # (3) LLM-as-a-judge: ask anything, i.e. if answer makes sense https://docs.evidentlyai.com/user-guide/customization/llm_as_a_judge
    # need to find out if this can only be applied on one column or on two as well to compare expected answer vs. actual answer
    prompt = """
        Please assess if the answer generated by an LLM LLM_ANSWER is suitable to answer a given question: QUESTION

        Use the following categories to judge the quality:
        good if the llm_answer is suitable to answer the question
        ok if the llm_answer is partly suitable to answer the question
        bad if the llm_answer is not suitable to answer the question
        Return only one category.
    """
    report = Report(metrics=[
        TextEvals(column_name="llm_answer", descriptors=[
            OpenAIPrompting(
                prompt=prompt,
                prompt_replace_string="LLM_ANSWER",
                context_replace_string="QUESTION",
                context_column="question",
                model="gpt-3.5-turbo-instruct",
                feature_type="cat",
                display_name="Assess llm answer towards initial question"
            )
    ])])
    report.run(reference_data=None, current_data=ground_truth_data_sample, column_mapping=column_mapping)
    report_df = report.as_dataframe()
    print(report_df.info())
    print(report_df)

    #report_df.to_csv("metrics_report.csv")


if __name__ == "__main__":
    es_client = setup_es_client_and_index(index_name=ES_INDEX_NAME)
    #dump_doc_embeddings_to_db(es_client=es_client, index_name=ES_INDEX_NAME)
    #extend_ground_truth_dataset(es_client=es_client, index_name=ES_INDEX_NAME)
    result = elastic_search_fields(es_client=es_client, index_name=ES_INDEX_NAME, search_query={
        "query": {
            "bool": {
                "must": [
                    {"exists": {"field": "text_vector"}},
                    {"exists": {"field": "llm_answer_vector"}}
                ]
            }
        }
    })
    #compute_cosine_similarity(result)
    
    # compute_metrics(pd.read_csv("ground-truth-data.csv",
    #                 usecols=["question", "contexts", "text", "llm_answer"]))
    # create_metrics_db(postgres_db_params=POSTGRES_DB_PARAMS)
    # create_metrics_table(postgres_db_params=POSTGRES_DB_PARAMS)
    # store_metrics(DB_PARAMS, metrics_df)