{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d04bcbe1",
   "metadata": {},
   "source": [
    "[Homework 1]('https://colab.research.google.com/github/Jaguar838/llm-zoomcamp/blob/main/HW/2025/01-intro/hw-01.ipynb')\n",
    "\n",
    "In this homework, we'll learn more about search and use Elastic Search for practice. \n",
    "\n",
    "> It's possible that your answers won't match exactly. If it's the case, select the closest one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d96f2e4",
   "metadata": {},
   "source": [
    "## Q1. Running Elastic \n",
    "\n",
    "Run Elastic Search 8.17.6, and get the cluster information. If you run it on localhost, this is how you do it:\n",
    "\n",
    "```bash\n",
    "curl localhost:9200\n",
    "```\n",
    "\n",
    "What's the `version.build_hash` value?\n",
    "\n",
    "Download and run Elastic Search docker-image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2540543b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.17.6: Pulling from elasticsearch/elasticsearch\n",
      "Digest: sha256:d954ee9958a1c5ff4098820fefdaca32c0cf19c573662ec89ba04bd122c19901\n",
      "Status: Image is up to date for docker.elastic.co/elasticsearch/elasticsearch:8.17.6\n",
      "docker.elastic.co/elasticsearch/elasticsearch:8.17.6\n",
      "docker.elastic.co/elasticsearch/elasticsearch:8.17.6\n"
     ]
    }
   ],
   "source": [
    "!docker pull docker.elastic.co/elasticsearch/elasticsearch:8.17.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28e7a719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docker: Error response from daemon: failed to set up container networking: driver failed programming external connectivity on endpoint cranky_newton (b6484a17f915181b4d415a8afca1af0bcdfa2f41572f49d23b4c68ee84f77321): Bind for 0.0.0.0:9200 failed: port is already allocated\n",
      "\n",
      "Run 'docker run --help' for more information\n"
     ]
    }
   ],
   "source": [
    "!docker run -p 9200:9200 -e \"discovery.type=single-node\" \\\n",
    "  -e \"xpack.security.enabled=false\" \\\n",
    "  docker.elastic.co/elasticsearch/elasticsearch:8.17.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05fdda22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\" : \"3ac3ed492c87\",\n",
      "  \"cluster_name\" : \"docker-cluster\",\n",
      "  \"cluster_uuid\" : \"XKV5WOh1QvSwpMNKIL0zNg\",\n",
      "  \"version\" : {\n",
      "    \"number\" : \"8.17.6\",\n",
      "    \"build_flavor\" : \"default\",\n",
      "    \"build_type\" : \"docker\",\n",
      "    \"build_hash\" : \"dbcbbbd0bc4924cfeb28929dc05d82d662c527b7\",\n",
      "    \"build_date\" : \"2025-04-30T14:07:12.231372970Z\",\n",
      "    \"build_snapshot\" : false,\n",
      "    \"lucene_version\" : \"9.12.0\",\n",
      "    \"minimum_wire_compatibility_version\" : \"7.17.0\",\n",
      "    \"minimum_index_compatibility_version\" : \"7.0.0\"\n",
      "  },\n",
      "  \"tagline\" : \"You Know, for Search\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!curl localhost:9200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a3fbcb",
   "metadata": {},
   "source": [
    "Answer Q1: \"build_hash\" : \"dbcbbbd0bc4924cfeb28929dc05d82d662c527b7\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fa88a5",
   "metadata": {},
   "source": [
    "## Getting the data\n",
    "\n",
    "Now let's get the FAQ data. You can run this snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec37da1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon’t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
       " 'section': 'General course-related questions',\n",
       " 'question': 'Course - When will the course start?',\n",
       " 'course': 'data-engineering-zoomcamp'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests \n",
    "\n",
    "docs_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main/01-intro/documents.json?raw=1'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)\n",
    "\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e3fadb",
   "metadata": {},
   "source": [
    "## Q2. Indexing the data\n",
    "\n",
    "Index the data in the same way as was shown in the course videos. Make the `course` field a keyword and the rest should be text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54ea1b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: elasticsearch==8.18.1 in /home/oleg_yamgurov/llm-zoomcamp/.venv/lib/python3.12/site-packages (8.18.1)\n",
      "Requirement already satisfied: elastic-transport<9,>=8.15.1 in /home/oleg_yamgurov/llm-zoomcamp/.venv/lib/python3.12/site-packages (from elasticsearch==8.18.1) (8.17.1)\n",
      "Requirement already satisfied: python-dateutil in /home/oleg_yamgurov/llm-zoomcamp/.venv/lib/python3.12/site-packages (from elasticsearch==8.18.1) (2.9.0.post0)\n",
      "Requirement already satisfied: typing-extensions in /home/oleg_yamgurov/llm-zoomcamp/.venv/lib/python3.12/site-packages (from elasticsearch==8.18.1) (4.14.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.2 in /home/oleg_yamgurov/llm-zoomcamp/.venv/lib/python3.12/site-packages (from elastic-transport<9,>=8.15.1->elasticsearch==8.18.1) (2.5.0)\n",
      "Requirement already satisfied: certifi in /home/oleg_yamgurov/llm-zoomcamp/.venv/lib/python3.12/site-packages (from elastic-transport<9,>=8.15.1->elasticsearch==8.18.1) (2025.6.15)\n",
      "Requirement already satisfied: six>=1.5 in /home/oleg_yamgurov/llm-zoomcamp/.venv/lib/python3.12/site-packages (from python-dateutil->elasticsearch==8.18.1) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install elasticsearch==8.18.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5e8cfe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Elasticsearch(['http://localhost:9200'])>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es_client = Elasticsearch('http://localhost:9200') \n",
    "es_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b05fccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            \"section\": {\"type\": \"text\"},\n",
    "            \"question\": {\"type\": \"text\"},\n",
    "            \"course\": {\"type\": \"keyword\"} \n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "index_name = \"course-questions\"\n",
    "\n",
    "es_client.indices.create(index=index_name, body=index_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4c2de33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oleg_yamgurov/llm-zoomcamp/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 948/948 [00:09<00:00, 104.62it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "for doc in tqdm(documents):\n",
    "    es_client.index(index=index_name, document=doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e201f0f3-b380-43a5-95ee-62b1b79002d1",
   "metadata": {},
   "source": [
    "Which function do you use for adding your data to elastic?\n",
    "\n",
    "* `insert`\n",
    "* `index`\n",
    "* `put`\n",
    "* `add`\n",
    "\n",
    "Answer Q2: `index`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc2252f",
   "metadata": {},
   "source": [
    "## Q3. Searching\n",
    "\n",
    "Now let's search in our index. \n",
    "\n",
    "We will execute a query \"How do execute a command on a Kubernetes pod?\". \n",
    "\n",
    "Use only `question` and `text` fields and give `question` a boost of 4, and use `\"type\": \"best_fields\"`.\n",
    "\n",
    "What's the score for the top ranking result?\n",
    "\n",
    "* 84.50\n",
    "* 64.50\n",
    "* 44.50\n",
    "* 24.50\n",
    "\n",
    "Look at the `_score` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8d8ea88-7412-49c1-8a8e-44d0d0862a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'How do execute a command on a Kubernetes pod?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0dda2240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31.973522,\n",
       " [{'text': 'Install the astronomer-cosmos package as a dependency. (see Terraform example).\\nMake a new folder, dbt/, inside the dags/ folder of your Composer GCP bucket and copy paste your dbt-core project there. (see example)\\nEnsure your profiles.yml is configured to authenticate with a service account key. (see BigQuery example)\\nCreate a new DAG using the DbtTaskGroup class and a ProfileConfig specifying a profiles_yml_filepath that points to the location of your JSON key file. (see example)\\nYour dbt lineage graph should now appear as tasks inside a task group like this:',\n",
       "   'section': 'Course Management Form for Homeworks',\n",
       "   'question': 'How to run a dbt-core project as an Airflow Task Group on Google Cloud Composer using a service account JSON key',\n",
       "   'course': 'data-engineering-zoomcamp'}])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def elastic_search(query):\n",
    "    search_query = {\n",
    "        \"size\": 1,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": {\n",
    "                    \"multi_match\": {\n",
    "                        \"query\": query,\n",
    "                        \"fields\": [\"question^4\", \"text\"],\n",
    "                        \"type\": \"best_fields\"\n",
    "                    }\n",
    "                },\n",
    "                \"filter\": {\n",
    "                    \"term\": {\n",
    "                        \"course\": \"data-engineering-zoomcamp\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = es_client.search(index=index_name, body=search_query)\n",
    "    \n",
    "    result_docs = []\n",
    "    \n",
    "    for hit in response['hits']['hits']:\n",
    "        result_docs.append(hit['_source'])\n",
    "    \n",
    "    return response['hits']['hits'][0]['_score'], result_docs\n",
    "\n",
    "\n",
    "elastic_search(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3934ec8",
   "metadata": {},
   "source": [
    "## Q4. Filtering\n",
    "\n",
    "Now ask a different question: \"How do copy a file to a Docker container?\".\n",
    "\n",
    "This time we are only interested in questions from `machine-learning-zoomcamp`.\n",
    "\n",
    "Return 3 results. What's the 3rd question returned by the search engine?\n",
    "\n",
    "* How do I debug a docker container?\n",
    "* How do I copy files from a different folder into docker container’s working directory?\n",
    "* How do Lambda container images work?\n",
    "* How can I annotate a graph?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77253b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'How do copy a file to a Docker container?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a429d9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer Q4: How do I copy files from a different folder into docker container’s working directory?, score: 73.38676\n"
     ]
    }
   ],
   "source": [
    "def elastic_search(query):\n",
    "    search_query = {\n",
    "        \"size\": 3,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": {\n",
    "                    \"multi_match\": {\n",
    "                        \"query\": query,\n",
    "                        \"fields\": [\"question^4\", \"text\"],\n",
    "                        \"type\": \"best_fields\"\n",
    "                    }\n",
    "                },\n",
    "                \"filter\": {\n",
    "                    \"term\": {\n",
    "                        \"course\": \"machine-learning-zoomcamp\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = es_client.search(index=index_name, body=search_query)\n",
    "    \n",
    "    result_docs = []\n",
    "    \n",
    "    for hit in response['hits']['hits']:\n",
    "        result_docs.append(hit['_source'])\n",
    "    \n",
    "    return response['hits']['hits'][0]['_score'], result_docs\n",
    "\n",
    "max_score, res_docs = elastic_search(query)\n",
    "\n",
    "print(f\"Answer Q4: {res_docs[2]['question']}, score: {max_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97363da",
   "metadata": {},
   "source": [
    "## Q5. Building a prompt\n",
    "\n",
    "Now we're ready to build a prompt to send to an LLM. \n",
    "\n",
    "Take the records returned from Elasticsearch in Q4 and use this template to build the context. Separate context entries by two linebreaks (`\\n\\n`)\n",
    "```python\n",
    "context_template = \"\"\"\n",
    "Q: {question}\n",
    "A: {text}\n",
    "\"\"\".strip()\n",
    "```\n",
    "\n",
    "Now use the context you just created along with the \"How do copy a file to a Docker container?\" question \n",
    "to construct a prompt using the template below:\n",
    "\n",
    "```\n",
    "prompt_template = \"\"\"\n",
    "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\"\"\".strip()\n",
    "```\n",
    "\n",
    "What's the length of the resulting prompt? (use the `len` function)\n",
    "\n",
    "* 946\n",
    "* 1446\n",
    "* 1946\n",
    "* 2446"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47dfc3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer Q5: length of the prompt: 1447\n"
     ]
    }
   ],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT: \n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in search_results:\n",
    "        context = context + f\"Q: {doc['question']}\\nA: {doc['text']}\\n\\n\"    \n",
    "        \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt\n",
    "\n",
    "\n",
    "prompt = build_prompt(query, res_docs)\n",
    "\n",
    "print(f\"Answer Q5: length of the prompt: {len(prompt)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24529e30",
   "metadata": {},
   "source": [
    "## Q6. Tokens\n",
    "\n",
    "When we use the OpenAI Platform, we're charged by the number of \n",
    "tokens we send in our prompt and receive in the response.\n",
    "\n",
    "The OpenAI python package uses `tiktoken` for tokenization:\n",
    "\n",
    "```bash\n",
    "pip install tiktoken\n",
    "```\n",
    "\n",
    "Let's calculate the number of tokens in our query: \n",
    "\n",
    "```python\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "```\n",
    "\n",
    "Use the `encode` function. How many tokens does our prompt have?\n",
    "\n",
    "* 120\n",
    "* 220\n",
    "* 320\n",
    "* 420\n",
    "\n",
    "Note: to decode back a token into a word, you can use the `decode_single_token_bytes` function:\n",
    "\n",
    "```python\n",
    "encoding.decode_single_token_bytes(63842)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4745eaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer Q6: num tokens: 321\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "tokens=encoding.encode(prompt)\n",
    "print(f\"Answer Q6: num tokens: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b164b12c",
   "metadata": {},
   "source": [
    "## Bonus: generating the answer (ungraded)\n",
    "\n",
    "Let's send the prompt to OpenAI. What's the response?  \n",
    "\n",
    "Note: you can replace OpenAI with Ollama. See module 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "82a3e8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Завантаження змінних середовища з файлу .env\n",
    "load_dotenv()\n",
    "# Убедитесь, что переменная окружения GROQ_API_KEY установлена\n",
    "api_key_groq = os.environ.get(\"GROQ_API_KEY\")\n",
    "\n",
    "if api_key_groq is None:\n",
    "    raise ValueError(\"API key is not set. Please set the GROQ_API_KEY environment variable.\")\n",
    "\n",
    "client_groq = Groq(api_key=api_key_groq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22295f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion = client_groq.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-8b-8192\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aa755a08-b98d-4e92-8994-04e6108499d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To execute a command on a Kubernetes pod, you can use the `kubectl exec` command. This is useful when you need to interact with a container directly, such as troubleshooting, inspecting files, or performing specific operations inside the pod. Here's how you can do it:\\n\\n### Basic Command\\n\\n```sh\\nkubectl exec -it <pod-name> -- <command>\\n```\\n\\n- `-it` is used for interactive tasks. The `i` flag stands for interactive, and the `t` flag allocates a TTY.\\n- `<pod-name>` is the name of the pod you want to interact with.\\n- `<command>` is the command you want to execute inside the pod.\\n\\n### Example\\n\\nIf you want to open a shell inside a pod named `my-pod`:\\n\\n```sh\\nkubectl exec -it my-pod -- /bin/bash\\n```\\n\\nIf the container uses `sh` instead of `bash`, you can use:\\n\\n```sh\\nkubectl exec -it my-pod -- /bin/sh\\n```\\n\\n### Specify a Container\\n\\nIf your pod has multiple containers, you'll need to specify the container name using the `-c` option:\\n\\n```sh\\nkubectl exec -it <pod-name> -c <container-name> -- <command>\\n```\\n\\n### Execute a Simple Command\\n\\nTo run a simple command, like listing files in a directory, you can do:\\n\\n```sh\\nkubectl exec my-pod -- ls /path/to/directory\\n```\\n\\n### Execute Non-Interactive Commands\\n\\nIf you don't need an interactive shell, you can omit the `-it`:\\n\\n```sh\\nkubectl exec <pod-name> -- <command>\\n```\\n\\n### Note\\n\\nMake sure you have the necessary permissions to execute commands on the targeted pod, and ensure that `kubectl` is configured to communicate with the correct Kubernetes cluster context.\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import httpx as httpx\n",
    "\n",
    "proxy = os.environ.get('HTTP_PROXY')\n",
    "api_key_openai = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "client_openai = OpenAI(http_client=httpx.Client(proxy=proxy), api_key=api_key_openai)\n",
    "response = client_openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=[{\"role\": \"user\", \"content\": q}]\n",
    ")\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b21237c3-80e9-429c-a089-d45428087046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_search(query):\n",
    "    search_query = {\n",
    "        \"size\": 3,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": {\n",
    "                    \"multi_match\": {\n",
    "                        \"query\": query,\n",
    "                        \"fields\": [\"question^4\", \"text\"],\n",
    "                        \"type\": \"best_fields\"\n",
    "                    }\n",
    "                },\n",
    "                \"filter\": {\n",
    "                    \"term\": {\n",
    "                        \"course\": \"machine-learning-zoomcamp\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = es_client.search(index=index_name, body=search_query)\n",
    "    \n",
    "    result_docs = []\n",
    "    \n",
    "    for hit in response['hits']['hits']:\n",
    "        result_docs.append(hit['_source'])\n",
    "    \n",
    "    return result_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "46900267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_llama(prompt):\n",
    "    response = client_groq.chat.completions.create(\n",
    "        model='llama3-8b-8192',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "97d35dec-c25f-472d-b961-20d5c30902ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_gpt(prompt):\n",
    "    response = client_openai.chat.completions.create(\n",
    "        model='gpt-4o',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8602f40b-ad3b-49c9-b3cc-051a79c888bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'how do I run kafka?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7218351f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rag_llama(query):\n",
    "    search_results = elastic_search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm_llama(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cd2c53ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rag_gpt(query):\n",
    "    search_results = elastic_search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm_gpt(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd4497b-c5d5-4258-b950-6b35d1af4ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rag_gpt(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385b012f-4905-422d-8d7c-3d542dfe5a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " QUESTION: the course has already started, can I still enroll?\n",
      "\n",
      "ANSWER: Yes, even if you don't register, you're still eligible to submit the homeworks.\n",
      "\n",
      "TRANSLATION IN RUSSIAN:\n",
      "Да, даже если вы не зарегистрировались, вы по-прежнему можете отправлять домашние задания.\n"
     ]
    }
   ],
   "source": [
    "print(rag('the course has already started, can I still enroll?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8095274-c9cd-4fd5-80d2-069fc951834f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'How do execute a command on a Kubernetes pod?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7a66ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_index': 'course-questions',\n",
       "  '_id': 'jkpqiZcBfjM-EchMzfrW',\n",
       "  '_score': 44.50556,\n",
       "  '_source': {'text': 'Launch the container image in interactive mode and overriding the entrypoint, so that it starts a bash command.\\ndocker run -it --entrypoint bash <image>\\nIf the container is already running, execute a command in the specific container:\\ndocker ps (find the container-id)\\ndocker exec -it <container-id> bash\\n(Marcos MJD)',\n",
       "   'section': '5. Deploying Machine Learning Models',\n",
       "   'question': 'How do I debug a docker container?',\n",
       "   'course': 'machine-learning-zoomcamp'}},\n",
       " {'_index': 'course-questions',\n",
       "  '_id': 'HUpriZcBfjM-EchMC_sQ',\n",
       "  '_score': 35.433445,\n",
       "  '_source': {'text': 'Deploy and Access the Kubernetes Dashboard\\nLuke',\n",
       "   'section': '10. Kubernetes and TensorFlow Serving',\n",
       "   'question': 'Kubernetes-dashboard',\n",
       "   'course': 'machine-learning-zoomcamp'}},\n",
       " {'_index': 'course-questions',\n",
       "  '_id': 'rkpqiZcBfjM-EchM2_qM',\n",
       "  '_score': 33.70974,\n",
       "  '_source': {'text': 'You can copy files from your local machine into a Docker container using the docker cp command. Here\\'s how to do it:\\nIn the Dockerfile, you can provide the folder containing the files that you want to copy over. The basic syntax is as follows:\\nCOPY [\"src/predict.py\", \"models/xgb_model.bin\", \"./\"]\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tGopakumar Gopinathan',\n",
       "   'section': '5. Deploying Machine Learning Models',\n",
       "   'question': 'How do I copy files from a different folder into docker container’s working directory?',\n",
       "   'course': 'machine-learning-zoomcamp'}},\n",
       " {'_index': 'course-questions',\n",
       "  '_id': 'nUpqiZcBfjM-EchM1PpF',\n",
       "  '_score': 33.2635,\n",
       "  '_source': {'text': 'Problem description:\\nI started a web-server in terminal (command window, powershell, etc.). How can I run another python script, which makes a request to this server?\\nSolution description:\\nJust open another terminal (command window, powershell, etc.) and run a python script.\\nAlena Kniazeva',\n",
       "   'section': '5. Deploying Machine Learning Models',\n",
       "   'question': 'How to run a script while a web-server is working?',\n",
       "   'course': 'machine-learning-zoomcamp'}},\n",
       " {'_index': 'course-questions',\n",
       "  '_id': 'cEpqiZcBfjM-EchMwfoA',\n",
       "  '_score': 32.589073,\n",
       "  '_source': {'text': \"Matplotlib has a cool method to annotate where you could provide an X,Y point and annotate with an arrow and text. For example this will show an arrow pointing to the x,y point optimal threshold.\\nplt.annotate(f'Optimal Threshold: {optimal_threshold:.2f}\\\\nOptimal F1 Score: {optimal_f1_score:.2f}',\\nxy=(optimal_threshold, optimal_f1_score),\\nxytext=(0.3, 0.5),\\ntextcoords='axes fraction',\\narrowprops=dict(facecolor='black', shrink=0.05))\\nQuinn Avila\",\n",
       "   'section': '4. Evaluation Metrics for Classification',\n",
       "   'question': 'How can I annotate a graph?',\n",
       "   'course': 'machine-learning-zoomcamp'}}]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def elastic_search(query):\n",
    "    search_query = {\n",
    "        \"size\": 5,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": {\n",
    "                    \"multi_match\": {\n",
    "                        \"query\": query,\n",
    "                        \"fields\": [\"question^4\", \"text\"],\n",
    "                        \"type\": \"best_fields\"\n",
    "                    }\n",
    "                },\n",
    "\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = es_client.search(index=index_name, body=search_query)\n",
    "    \n",
    "    result_docs = []\n",
    "    \n",
    "    for hit in response['hits']['hits']:\n",
    "        result_docs.append(hit['_source'])\n",
    "    \n",
    "    return response['hits']['hits']\n",
    "\n",
    "elastic_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a71d9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'How do copy a file to a Docker container?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a38388",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_search(query):\n",
    "    search_query = {\n",
    "        \"size\": 3,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": {\n",
    "                    \"multi_match\": {\n",
    "                        \"query\": query,\n",
    "                        \"fields\": [\"question^4\", \"text\"],\n",
    "                        \"type\": \"best_fields\"\n",
    "                    }\n",
    "                },\n",
    "                \"filter\": {\n",
    "                    \"term\": {\n",
    "                        \"course\": \"machine-learning-zoomcamp\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = es_client.search(index=index_name, body=search_query)\n",
    "    \n",
    "    result_docs = []\n",
    "    \n",
    "    for hit in response['hits']['hits']:\n",
    "        result_docs.append(hit['_source'])\n",
    "    \n",
    "    return response['hits']['hits'][0]['_score'], result_docs\n",
    "\n",
    "max_score, res_docs = elastic_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5e6e2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How do I copy files from a different folder into docker container’s working directory?'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_docs[2]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a677b17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"How do copy a file to a Docker container?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12676ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT: \n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in search_results:\n",
    "        context = context + f\"Q: {doc['question']}\\nA: {doc['text']}\\n\\n\"    \n",
    "        \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05385501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the prompt:  1447\n"
     ]
    }
   ],
   "source": [
    "prompt = build_prompt(query, res_docs)\n",
    "\n",
    "print(\"Length of the prompt: \", len(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5d32aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You\\'re a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\\nUse only the facts from the CONTEXT when answering the QUESTION.\\n\\nQUESTION: How do copy a file to a Docker container?\\n\\nCONTEXT: \\nQ: How do I debug a docker container?\\nA: Launch the container image in interactive mode and overriding the entrypoint, so that it starts a bash command.\\ndocker run -it --entrypoint bash <image>\\nIf the container is already running, execute a command in the specific container:\\ndocker ps (find the container-id)\\ndocker exec -it <container-id> bash\\n(Marcos MJD)\\n\\nQ: How do I copy files from my local machine to docker container?\\nA: You can copy files from your local machine into a Docker container using the docker cp command. Here\\'s how to do it:\\nTo copy a file or directory from your local machine into a running Docker container, you can use the `docker cp command`. The basic syntax is as follows:\\ndocker cp /path/to/local/file_or_directory container_id:/path/in/container\\nHrithik Kumar Advani\\n\\nQ: How do I copy files from a different folder into docker container’s working directory?\\nA: You can copy files from your local machine into a Docker container using the docker cp command. Here\\'s how to do it:\\nIn the Dockerfile, you can provide the folder containing the files that you want to copy over. The basic syntax is as follows:\\nCOPY [\"src/predict.py\", \"models/xgb_model.bin\", \"./\"]\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tGopakumar Gopinathan'"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ac8098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d94a603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num tokens:  321\n"
     ]
    }
   ],
   "source": [
    "encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "tokens=encoding.encode(prompt)\n",
    "print('Num tokens: ', len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c75d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b\"You're\""
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.decode_single_token_bytes(63842)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8bb8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context, to copy a file to a Docker container, you can use the `docker cp` command. The basic syntax is as follows:\n",
      "\n",
      "`docker cp /path/to/local/file_or_directory container_id:/path/in/container`\n",
      "\n",
      "Replace `/path/to/local/file_or_directory` with the path to the file you want to copy, and `container_id` with the ID of the container you want to copy the file to.\n"
     ]
    }
   ],
   "source": [
    "output = llm(prompt)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d6a3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Прайсы за 1000 токенов ($)\n",
    "MODEL_PRICING = {\n",
    "    \"gpt-4.1\": {\"prompt\": 0.0020, \"cached\": 0.0005, \"output\": 0.0080},\n",
    "    \"gpt-4.1-mini\": {\"prompt\": 0.0004, \"cached\": 0.0001, \"output\": 0.0016},\n",
    "    \"gpt-4.1-nano\": {\"prompt\": 0.0001, \"cached\": 0.000025, \"output\": 0.0004},\n",
    "    \"openai-o3\": {\"prompt\": 0.0020, \"cached\": 0.0005, \"output\": 0.0080},\n",
    "    \"openai-o4-mini\": {\"prompt\": 0.0011, \"cached\": 0.000275, \"output\": 0.0044}\n",
    "}\n",
    "\n",
    "def estimate_cost(prompt: str, output: str, model: str = \"gpt-4.1\", discount: float = 0.0, cached_input: bool = False):\n",
    "    if model not in MODEL_PRICING:\n",
    "        raise ValueError(f\"Модель '{model}' не найдена в прайс-листе\")\n",
    "\n",
    "    pricing = MODEL_PRICING[model]\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "\n",
    "    # Подсчёт токенов\n",
    "    prompt_tokens = len(encoding.encode(prompt))\n",
    "    output_tokens = len(encoding.encode(output))\n",
    "    total_tokens = prompt_tokens + output_tokens\n",
    "\n",
    "    # Выбор тарифа для prompt\n",
    "    prompt_rate = pricing[\"cached\"] if cached_input and \"cached\" in pricing else pricing[\"prompt\"]\n",
    "\n",
    "    # Расчёт стоимости\n",
    "    raw_prompt_cost = (prompt_tokens / 1_000) * prompt_rate\n",
    "    raw_output_cost = (output_tokens / 1_000) * pricing[\"output\"]\n",
    "    total_raw_cost = raw_prompt_cost + raw_output_cost\n",
    "    total_discounted = total_raw_cost * (1 - discount)\n",
    "\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"cached_input\": cached_input,\n",
    "        \"discount\": discount,\n",
    "        \"prompt_tokens\": prompt_tokens,\n",
    "        \"output_tokens\": output_tokens,\n",
    "        \"total_tokens\": total_tokens,\n",
    "        \"cost_prompt\": round(raw_prompt_cost, 6),\n",
    "        \"cost_output\": round(raw_output_cost, 6),\n",
    "        \"total_cost\": round(total_discounted, 6)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1fcb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: gpt-4.1-nano\n",
      "cached_input: True\n",
      "discount: 0.5\n",
      "prompt_tokens: 321\n",
      "output_tokens: 87\n",
      "total_tokens: 408\n",
      "cost_prompt: 8e-06\n",
      "cost_output: 3.5e-05\n",
      "total_cost: 2.1e-05\n"
     ]
    }
   ],
   "source": [
    "res = estimate_cost(\n",
    "    prompt=prompt,\n",
    "    output=output,\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    cached_input=True,\n",
    "    discount=0.5\n",
    ")\n",
    "\n",
    "for k, v in res.items():\n",
    "    print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c1ccfdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
