{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92cc24d9-582a-4cdf-b76c-b286212353cd",
   "metadata": {},
   "source": [
    "### Q2. Читання документів\n",
    "Тепер ми можемо читати документи. Створіть власний блок поглинання коду\n",
    "\n",
    "\n",
    "Давайте почитаємо документи. Використаємо той самий код, який ми використовували для розбору FAQ: [parse-faq-llm.ipynb](https://github.com/DataTalksClub/llm-zoomcamp/blob/main/cohorts/2024/05-orchestration/parse-faq-llm.ipynb)\n",
    "\n",
    "\n",
    "Використовуйте наступний ідентифікатор документа: ```1qZjwHkvP0lXHiE4zdbWyUXSVfmVGzougDD6N37bat3E```.\n",
    "\n",
    "\n",
    "Це ідентифікатор документа [LLM FAQ версія 1](https://docs.google.com/document/d/1qZjwHkvP0lXHiE4zdbWyUXSVfmVGzougDD6N37bat3E/edit)\n",
    "\n",
    "\n",
    "Скопіюйте код до редактора. Скільки документів FAQ ми опрацювали?\n",
    "\n",
    "\n",
    "* 1\n",
    "* 2\n",
    "* 3\n",
    "* 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ebdfd26-8ce6-44c9-831d-e701b1b1b305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "import requests\n",
    "import docx\n",
    "import hashlib\n",
    "\n",
    "from datetime import datetime\n",
    "from elasticsearch import Elasticsearch\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a8e80c5-a587-4028-b3c9-cf289b42f075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_line(line):\n",
    "    line = line.strip()\n",
    "    line = line.strip('\\uFEFF')\n",
    "    return line\n",
    "\n",
    "def read_faq(file_id):\n",
    "    url = f'https://docs.google.com/document/d/{file_id}/export?format=docx'\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    with io.BytesIO(response.content) as f_in:\n",
    "        doc = docx.Document(f_in)\n",
    "\n",
    "    questions = []\n",
    "\n",
    "    question_heading_style = 'heading 2'\n",
    "    section_heading_style = 'heading 1'\n",
    "    \n",
    "    heading_id = ''\n",
    "    section_title = ''\n",
    "    question_title = ''\n",
    "    answer_text_so_far = ''\n",
    "     \n",
    "    for p in doc.paragraphs:\n",
    "        style = p.style.name.lower()\n",
    "        p_text = clean_line(p.text)\n",
    "    \n",
    "        if len(p_text) == 0:\n",
    "            continue\n",
    "    \n",
    "        if style == section_heading_style:\n",
    "            section_title = p_text\n",
    "            continue\n",
    "    \n",
    "        if style == question_heading_style:\n",
    "            answer_text_so_far = answer_text_so_far.strip()\n",
    "            if answer_text_so_far != '' and section_title != '' and question_title != '':\n",
    "                questions.append({\n",
    "                    'text': answer_text_so_far,\n",
    "                    'section': section_title,\n",
    "                    'question': question_title,\n",
    "                })\n",
    "                answer_text_so_far = ''\n",
    "    \n",
    "            question_title = p_text\n",
    "            continue\n",
    "        \n",
    "        answer_text_so_far += '\\n' + p_text\n",
    "    \n",
    "    answer_text_so_far = answer_text_so_far.strip()\n",
    "    if answer_text_so_far != '' and section_title != '' and question_title != '':\n",
    "        questions.append({\n",
    "            'text': answer_text_so_far,\n",
    "            'section': section_title,\n",
    "            'question': question_title,\n",
    "        })\n",
    "\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf6abf63-7e04-41bc-8c43-122c3ae44730",
   "metadata": {},
   "outputs": [],
   "source": [
    "faq_documents = {\n",
    "    # LLM Version 1\n",
    "    'llm-zoomcamp': '1qZjwHkvP0lXHiE4zdbWyUXSVfmVGzougDD6N37bat3E',\n",
    "    # LLM Version 2\n",
    "    # 'llm-zoomcamp': '1m2KexowAXTmexfC5rVTCSnaShvdUQ8Ag2IEiwBDHxN0',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca2b6a32-ceac-420b-820a-baa13c59c5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm-zoomcamp\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "\n",
    "for course, file_id in faq_documents.items():\n",
    "    print(course)\n",
    "    course_documents = read_faq(file_id)\n",
    "    documents.append({'course': course, 'documents': course_documents})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e215af89-4455-4e5e-9a96-0f2cfaff52ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2164fdb1-22eb-4340-a7bc-4866d5dc8e86",
   "metadata": {},
   "source": [
    "### Q3. Chunking\n",
    "We don't really need to do any chuncking because our documents already have well-specified boundaries. So we just need to return the documents without any changes.\n",
    "\n",
    "So let's go to the transformation part and add a custom code chunking block:\n",
    "```python\n",
    "documents = []\n",
    "\n",
    "for doc in data['documents']:\n",
    "    doc['course'] = data['course']\n",
    "    # previously we used just \"id\" for document ID\n",
    "    doc['document_id'] = generate_document_id(doc)\n",
    "    documents.append(doc)\n",
    "\n",
    "print(len(documents))\n",
    "\n",
    "return documents\n",
    "```\n",
    "Where data is the input parameter to the transformer.\n",
    "\n",
    "And the generate_document_id is defined in the same way as in module 4:\n",
    "```python\n",
    "import hashlib\n",
    "\n",
    "def generate_document_id(doc):\n",
    "    combined = f\"{doc['course']}-{doc['question']}-{doc['text'][:10]}\"\n",
    "    hash_object = hashlib.md5(combined.encode())\n",
    "    hash_hex = hash_object.hexdigest()\n",
    "    document_id = hash_hex[:8]\n",
    "    return document_id\n",
    "```\n",
    "Note: if instead of a single dictionary you get a list, add a for loop:\n",
    "```\n",
    "for course_dict in data:\n",
    "    ...\n",
    "```\n",
    "You can check the type of data with this code:\n",
    "\n",
    "```print(type(data))```\n",
    "\n",
    "How many documents (chunks) do we have in the output?\n",
    "\n",
    "* 66\n",
    "* 76\n",
    "* 86\n",
    "* 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e0a31c1-a7a8-4c7e-86a9-aef7ddfd6ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20060b35-f953-4f1c-b0f1-e142209cde7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9af7989c-e51e-462e-953e-a09b63cc3fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_document_id(doc):\n",
    "    combined = f\"{doc['course']}-{doc['question']}-{doc['text'][:10]}\"\n",
    "    hash_object = hashlib.md5(combined.encode())\n",
    "    hash_hex = hash_object.hexdigest()\n",
    "    document_id = hash_hex[:8]\n",
    "    return document_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ae655eb-c868-475c-998e-8aef648c7e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunking():\n",
    "    documents = []\n",
    "    \n",
    "    for doc in data['documents']:\n",
    "        doc['course'] = data['course']\n",
    "        # previously we used just \"id\" for document ID\n",
    "        doc['document_id'] = generate_document_id(doc)\n",
    "        documents.append(doc)\n",
    "    \n",
    "    print(len(documents))\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53c4618d-994f-4f81-a038-bc1fae776f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n"
     ]
    }
   ],
   "source": [
    "documents = chunking()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db811163-e5ee-4a13-b69d-d16aa8b62e1d",
   "metadata": {},
   "source": [
    "### Tokenization and embeddings\n",
    "We don't need any tokenization, so we skip it.\n",
    "\n",
    "Because currently it's required in mage, we can create a dummy code block:\n",
    "\n",
    "Create a custom code block\n",
    "Don't change it\n",
    "Because we will use text search, we also don't need embeddings, so skip it too.\n",
    "\n",
    "If you want to use sentence transformers - the ones from module 3 - you don't need tokenization, but need embeddings (you don't need it for this homework)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281e2687-8bd5-4de0-866d-96f422063f12",
   "metadata": {},
   "source": [
    "### Q4. Export\n",
    "Now we're ready to index the data with elasticsearch. For that, we use the Export part of the pipeline\n",
    "\n",
    "* Go to the Export part\n",
    "* Select vector databases -> Elasticsearch\n",
    "* Open the code for editing\n",
    "Because we won't use vector search, but usual text search, we will need to adjust the code.\n",
    "\n",
    "First, let's change the line where we read the index name:\n",
    "\n",
    "```index_name = kwargs.get('index_name', 'documents')```\n",
    "\n",
    "To ```index_name_prefix``` - we will parametrize it with the day and time we run the pipeline\n",
    "```python\n",
    "from datetime import datetime\n",
    "\n",
    "index_name_prefix = kwargs.get('index_name', 'documents')\n",
    "current_time = datetime.now().strftime(\"%Y%m%d_%M%S\")\n",
    "index_name = f\"{index_name_prefix}_{current_time}\"\n",
    "print(\"index name:\", index_name)\n",
    "```\n",
    "We will need to save the name in a global variable, so it can be accessible in other code blocks\n",
    "```python\n",
    "from mage_ai.data_preparation.variable_manager import set_global_variable\n",
    "\n",
    "set_global_variable('YOUR_PIPELINE_NAME', 'index_name', index_name)\n",
    "```\n",
    "Where your pipeline name is the name of the pipeline, e.g. transcendent_nexus (replace the space with underscore _)\n",
    "\n",
    "Replace index settings with the settings we used previously:\n",
    "```python\n",
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": number_of_shards,\n",
    "        \"number_of_replicas\": number_of_replicas\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            \"section\": {\"type\": \"text\"},\n",
    "            \"question\": {\"type\": \"text\"},\n",
    "            \"course\": {\"type\": \"keyword\"},\n",
    "            \"document_id\": {\"type\": \"keyword\"}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "Remove the embeddings line:\n",
    "```python\n",
    "if isinstance(document[vector_column_name], np.ndarray):\n",
    "    document[vector_column_name] = document[vector_column_name].tolist()\n",
    "```\n",
    "At the end (outside of the indexing for loop), print the last document:\n",
    "```python\n",
    "print(document)\n",
    "```\n",
    "Now execute the block.\n",
    "\n",
    "What's the last document id?\n",
    "\n",
    "Also note the index name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bba1dff9-caae-4ccd-bfa9-4ab913b8d89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bober\\AppData\\Local\\Temp\\ipykernel_9924\\2442128961.py:24: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  es_client.indices.create(index=index_name, body=index_settings)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'documents_20240820_4946'})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_client = Elasticsearch('http://localhost:9200') \n",
    "\n",
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            \"section\": {\"type\": \"text\"},\n",
    "            \"question\": {\"type\": \"text\"},\n",
    "            \"course\": {\"type\": \"keyword\"},\n",
    "            \"id\": {\"type\": \"keyword\"},\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "index_name_prefix = \"documents\"\n",
    "current_time = datetime.now().strftime(\"%Y%m%d_%M%S\")\n",
    "index_name = f\"{index_name_prefix}_{current_time}\"\n",
    "\n",
    "es_client.indices.delete(index=index_name, ignore_unavailable=True)\n",
    "es_client.indices.create(index=index_name, body=index_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f37f1acd-51bc-4859-ab43-9393d59cf72d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23482130a51d4c2e9666d20ca2efc13d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/86 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for doc in tqdm(documents):\n",
    "    es_client.index(index=index_name, document=doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8cf9dc7-0ac1-47ca-a650-d6a621b8bdeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Answer',\n",
       " 'section': 'Workshops: X',\n",
       " 'question': 'Question',\n",
       " 'course': 'llm-zoomcamp',\n",
       " 'document_id': 'd8c4c7bb'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5310e2f6-f35c-459a-9f3c-94d96e8551ef",
   "metadata": {},
   "source": [
    "### Q5. Testing the retrieval\n",
    "Now let's test the retrieval. Use mage or jupyter notebook to test it.\n",
    "\n",
    "Let's use the following query: \"When is the next cohort?\"\n",
    "\n",
    "What's the ID of the top matching result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "360f1d91-7591-4d75-b17d-033e9e630123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_search(query, course='llm-zoomcamp'):\n",
    "    search_query = {\n",
    "        \"size\": 5,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": {\n",
    "                    \"multi_match\": {\n",
    "                        \"query\": query,\n",
    "                        \"fields\": [\"question^3\", \"text\", \"section\"],\n",
    "                        \"type\": \"best_fields\"\n",
    "                    }\n",
    "                },\n",
    "                # \"filter\": {\n",
    "                #     \"term\": {\n",
    "                #         \"course\": course\n",
    "                #     }\n",
    "                # }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = es_client.search(index='documents_20240820_2525', body=search_query)\n",
    "    \n",
    "    result_docs = []\n",
    "    \n",
    "    for hit in response['hits']['hits']:\n",
    "        result_docs.append(hit['_source'])\n",
    "    \n",
    "    return result_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16c48562-c670-45bf-b9da-e48b9383729f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bober\\AppData\\Local\\Temp\\ipykernel_9924\\1868048032.py:22: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  response = es_client.search(index='documents_20240820_2525', body=search_query)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'text': 'Summer 2025 (via Alexey).',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'When will the course be offered next?',\n",
       "  'course': 'llm-zoomcamp',\n",
       "  'document_id': 'bf024675'},\n",
       " {'text': 'Cosine similarity is a measure used to calculate the similarity between two non-zero vectors, often used in text analysis to determine how similar two documents are based on their content. This metric computes the cosine of the angle between two vectors, which are typically word counts or TF-IDF values of the documents. The cosine similarity value ranges from -1 to 1, where 1 indicates that the vectors are identical, 0 indicates that the vectors are orthogonal (no similarity), and -1 represents completely opposite vectors.',\n",
       "  'section': 'Module 3: X',\n",
       "  'question': 'What is the cosine similarity?',\n",
       "  'course': 'llm-zoomcamp',\n",
       "  'document_id': 'ee355823'},\n",
       " {'text': 'The error indicates that you have not changed all instances of “employee_handbook” to “homework” in your pipeline settings',\n",
       "  'section': 'Workshops: dlthub',\n",
       "  'question': 'There is an error when opening the table using dbtable = db.open_table(\"notion_pages___homework\"): FileNotFoundError: Table notion_pages___homework does not exist.Please first call db.create_table(notion_pages___homework, data)',\n",
       "  'course': 'llm-zoomcamp',\n",
       "  'document_id': '6cf805ca'},\n",
       " {'text': 'Make sure you open the correct table in line 3: dbtable = db.open_table(\"notion_pages___homework\")T',\n",
       "  'section': 'Workshops: dlthub',\n",
       "  'question': 'There is an error when running main(): FileNotFoundError: Table notion_pages___homework does not exist.Please first call db.create_table(notion_pages___homework, data)',\n",
       "  'course': 'llm-zoomcamp',\n",
       "  'document_id': 'e18124d4'},\n",
       " {'text': 'This course is being offered for the first time, and things will keep changing until a given module is ready, at which point it shall be announced. Working on the material/homework in advance will be at your own risk, as the final version could be different.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'I was working on next week’s homework/content - why does it keep changing?',\n",
       "  'course': 'llm-zoomcamp',\n",
       "  'document_id': 'fb81c6ff'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = elastic_search(\n",
    "    query=\"When is the next cohort?\"\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa93f011-bfa3-4ac8-9e8d-a3d14867c4c9",
   "metadata": {},
   "source": [
    "### Q6. Reindexing\n",
    "Наш документ FAQ змінюється: кожного дня учасники курсу додають нові записи або покращують вже існуючі.\n",
    "\n",
    "Уявіть, що минув певний час і документ змінився. Для цього у нас є ще одна версія документа FAQ: версія 2.\n",
    "\n",
    "The ID of this document is ```1T3MdwUvqCL3jrh3d3VCXQ8xE0UqRzI3bfgpfBq3ZWG0.```\n",
    "\n",
    "Повторно виконаємо весь пайплайн з оновленими даними.\n",
    "\n",
    "Для того ж самого запиту \"Коли наступна когорта?\". Який ідентифікатор найкращого результату?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60b4d8e-fb46-4c6a-872b-f6013d302a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
