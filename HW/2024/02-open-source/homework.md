## Домашнє завдання: Відкриті LLM

У цьому домашньому завданні ми будемо експериментувати з Ollama.

> Ймовірно, ваші відповіді не будуть точними. Якщо так, виберіть найближчий варіант.

## Q1. Запуск Ollama з Docker

Запустимо Ollama за допомогою Docker. Нам потрібно виконати
той самий команду, що і в лекціях:

```bash
docker run -it \
    --rm \
    -v ollama:/root/.ollama \
    -p 11434:11434 \
    --name ollama \
    ollama/ollama
```

Яка версія Ollama клієнта?

Щоб дізнатися, увійдіть у контейнер і виконайте `ollama` з прапором `-v`.

## Q2. Завантаження LLM

Ми завантажимо менший LLM - gemma:2b.

Знову увійдіть у контейнер і витягніть модель:

```bash
ollama pull gemma:2b
```

У Docker, результати збережені у `/root/.ollama`.

Нас цікавлять метадані про цю модель. Ви можете знайти їх у `models/manifests/registry.ollama.ai/library`.

Який вміст файлу, що відноситься до gemma?

## Q3. Запуск LLM

Протестуйте наступний промпт: "10 * 10". Яка відповідь?

## Q4. Завантаження вагів

Ми не хочемо витягувати ваги кожного разу при запуску контейнера.
Давайте зробимо це один раз і мати їх доступними кожного разу при запуску контейнера.

Спочатку потрібно змінити спосіб запуску контейнера.

Замість прив’язки папки `/root/.ollama` до іменованого тому,
давайте прив’яжемо її до локальної директорії:

```bash
mkdir ollama_files

docker run -it \
    --rm \
    -v ./ollama_files:/root/.ollama \
    -p 11434:11434 \
    --name ollama \
    ollama/ollama
```

Тепер витягніть модель:

```bash
docker exec -it ollama ollama pull gemma:2b 
```

Який розмір папки `ollama_files/models`?

* 0.6G
* 1.2G
* 1.7G
* 2.2G

Підказка: на Linux можна використовувати `du -h` для цього.

## Q5. Додавання вагів

Тепер зупинимо контейнер і додамо ваги
до нового образу.

Для цього створимо `Dockerfile`:

```dockerfile
FROM ollama/ollama

COPY ...
```

Що ви додасте після `COPY`?

## Q6. Запуск

Зберемо його:

```bash
docker build -t ollama-gemma2b .
```

І запустимо:

```bash
docker run -it --rm -p 11434:11434 ollama-gemma2b
```

Ми можемо підключитися до нього за допомогою клієнта OpenAI.

Протестуємо його з наступним промптом:

```python
prompt = "What's the formula for energy?"
```

Також, щоб результати були відтворюваними, встановіть параметр `temperature` на 0:

```bash
response = client.chat.completions.create(
    #...
    temperature=0.0
)
```

Скільки токенів завершення ви отримали у відповіді?

* 304
* 604
* 904
* 1204

## Надішліть результати

* Надішліть ваші результати сюди: https://courses.datatalks.club/llm-zoomcamp-2024/homework/hw2
* Ймовірно, ваші відповіді не будуть точними. Якщо так, виберіть найближчий варіант.
