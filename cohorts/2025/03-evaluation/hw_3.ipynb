{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46044cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting minsearch\n",
      "  Downloading minsearch-0.0.4-py3-none-any.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: qdrant_client in /home/martin/miniconda3/envs/llmcamp/lib/python3.10/site-packages (1.14.2)\n",
      "Collecting qdrant_client\n",
      "  Downloading qdrant_client-1.14.3-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: rouge in /home/martin/miniconda3/envs/llmcamp/lib/python3.10/site-packages (1.0.1)\n",
      "Requirement already satisfied: scikit-learn in /home/martin/miniconda3/envs/llmcamp/lib/python3.10/site-packages (1.6.1)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: tqdm in /home/martin/.local/lib/python3.10/site-packages (4.66.4)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: requests in /home/martin/.local/lib/python3.10/site-packages (2.31.0)\n",
      "Collecting requests\n",
      "  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: pandas in /home/martin/miniconda3/envs/llmcamp/lib/python3.10/site-packages (2.2.3)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: numpy in /home/martin/miniconda3/envs/llmcamp/lib/python3.10/site-packages (2.2.6)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in /home/martin/miniconda3/envs/llmcamp/lib/python3.10/site-packages (from qdrant_client) (1.73.0)\n",
      "Requirement already satisfied: httpx>=0.20.0 in /home/martin/.local/lib/python3.10/site-packages (from httpx[http2]>=0.20.0->qdrant_client) (0.27.0)\n",
      "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in /home/martin/miniconda3/envs/llmcamp/lib/python3.10/site-packages (from qdrant_client) (2.10.1)\n",
      "Requirement already satisfied: protobuf>=3.20.0 in /home/martin/miniconda3/envs/llmcamp/lib/python3.10/site-packages (from qdrant_client) (6.31.1)\n",
      "Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8 in /home/martin/miniconda3/envs/llmcamp/lib/python3.10/site-packages (from qdrant_client) (2.11.5)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.14 in /home/martin/miniconda3/envs/llmcamp/lib/python3.10/site-packages (from qdrant_client) (2.4.0)\n",
      "Requirement already satisfied: six in /home/martin/miniconda3/envs/llmcamp/lib/python3.10/site-packages (from rouge) (1.17.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/martin/miniconda3/envs/llmcamp/lib/python3.10/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/martin/.local/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/martin/.local/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/martin/.local/lib/python3.10/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/martin/.local/lib/python3.10/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/martin/.local/lib/python3.10/site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/martin/.local/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/martin/.local/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/martin/.local/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: anyio in /home/martin/.local/lib/python3.10/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant_client) (4.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/martin/.local/lib/python3.10/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant_client) (1.0.5)\n",
      "Requirement already satisfied: sniffio in /home/martin/.local/lib/python3.10/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant_client) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/martin/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant_client) (0.14.0)\n",
      "Requirement already satisfied: h2<5,>=3 in /home/martin/miniconda3/envs/llmcamp/lib/python3.10/site-packages (from httpx[http2]>=0.20.0->qdrant_client) (4.2.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in /home/martin/miniconda3/envs/llmcamp/lib/python3.10/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant_client) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.1 in /home/martin/miniconda3/envs/llmcamp/lib/python3.10/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant_client) (4.1.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/martin/.local/lib/python3.10/site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant_client) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/martin/miniconda3/envs/llmcamp/lib/python3.10/site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant_client) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /home/martin/miniconda3/envs/llmcamp/lib/python3.10/site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant_client) (4.14.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/martin/miniconda3/envs/llmcamp/lib/python3.10/site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant_client) (0.4.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/martin/.local/lib/python3.10/site-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant_client) (1.2.1)\n",
      "Downloading minsearch-0.0.4-py3-none-any.whl (11 kB)\n",
      "Downloading qdrant_client-1.14.3-py3-none-any.whl (328 kB)\n",
      "Downloading scikit_learn-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Downloading pandas-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, requests, scikit-learn, pandas, minsearch, qdrant_client\n",
      "\u001b[2K  Attempting uninstall: tqdm\n",
      "\u001b[2K    Found existing installation: tqdm 4.66.4\n",
      "\u001b[2K    Uninstalling tqdm-4.66.4:\n",
      "\u001b[2K      Successfully uninstalled tqdm-4.66.4\n",
      "\u001b[2K  Attempting uninstall: requests\n",
      "\u001b[2K    Found existing installation: requests 2.31.0\n",
      "\u001b[2K    Uninstalling requests-2.31.0:\n",
      "\u001b[2K      Successfully uninstalled requests-2.31.0\n",
      "\u001b[2K  Attempting uninstall: scikit-learn2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/6\u001b[0m [requests]\n",
      "\u001b[2K    Found existing installation: scikit-learn 1.6.1[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/6\u001b[0m [requests]\n",
      "\u001b[2K    Uninstalling scikit-learn-1.6.1:0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [scikit-learn]\n",
      "\u001b[2K      Successfully uninstalled scikit-learn-1.6.1m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [scikit-learn]\n",
      "\u001b[2K  Attempting uninstall: pandas━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [scikit-learn]\n",
      "\u001b[2K    Found existing installation: pandas 2.2.3╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [scikit-learn]\n",
      "\u001b[2K    Uninstalling pandas-2.2.3:━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/6\u001b[0m [pandas]n]\n",
      "\u001b[2K      Successfully uninstalled pandas-2.2.338;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/6\u001b[0m [pandas]\n",
      "\u001b[2K  Attempting uninstall: qdrant_client━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/6\u001b[0m [pandas]\n",
      "\u001b[2K    Found existing installation: qdrant-client 1.14.2\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/6\u001b[0m [pandas]\n",
      "\u001b[2K    Uninstalling qdrant-client-1.14.2:[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/6\u001b[0m [pandas]\n",
      "\u001b[2K      Successfully uninstalled qdrant-client-1.14.2m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/6\u001b[0m [pandas]\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [qdrant_client]m \u001b[32m5/6\u001b[0m [qdrant_client]\n",
      "\u001b[1A\u001b[2KSuccessfully installed minsearch-0.0.4 pandas-2.3.1 qdrant_client-1.14.3 requests-2.32.4 scikit-learn-1.7.0 tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install -U minsearch qdrant_client rouge scikit-learn tqdm requests pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cdf7ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, pandas as pd, numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# # Datos\n",
    "# url_prefix = 'https://raw.githubusercontent.com/DataTalksClub/llm-zoomcamp/main/03-evaluation/'\n",
    "# docs_url = url_prefix + 'search_evaluation/documents-with-ids.json'\n",
    "# ground_truth_url = url_prefix + 'search_evaluation/ground-truth-data.csv'\n",
    "\n",
    "# documents       = requests.get(docs_url).json()\n",
    "# df_ground_truth = pd.read_csv(ground_truth_url)\n",
    "# ground_truth    = df_ground_truth.to_dict(orient='records')\n",
    "\n",
    "# Métricas de evaluación\n",
    "def hit_rate(relevance_total):\n",
    "    return sum(any(line) for line in relevance_total) / len(relevance_total)\n",
    "\n",
    "def mrr(relevance_total):\n",
    "    score = 0.0\n",
    "    for line in relevance_total:\n",
    "        for rank, rel in enumerate(line, 1):\n",
    "            if rel:\n",
    "                score += 1 / rank\n",
    "                break\n",
    "    return score / len(relevance_total)\n",
    "\n",
    "def evaluate(ground_truth, search_function):\n",
    "    relevance_total = []\n",
    "    for q in tqdm(ground_truth):\n",
    "        doc_id  = q['document']\n",
    "        results = search_function(q)\n",
    "        relevance_total.append([d['id'] == doc_id for d in results])\n",
    "    return {'hit_rate': hit_rate(relevance_total), 'mrr': mrr(relevance_total)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aed3455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs: 948, GT pairs: 4627\n"
     ]
    }
   ],
   "source": [
    "import json, pandas as pd, numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# Carpeta donde están los archivos ('' = directorio actual)\n",
    "DATA_DIR = Path(\"\")          # o Path(\"data\") si los tienes en ./data/\n",
    "\n",
    "# ---------- Documentos del FAQ ----------\n",
    "with open(DATA_DIR / \"documents-with-ids.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    documents = json.load(f)\n",
    "\n",
    "# ---------- Ground-truth (pregunta → doc_id correcto) ----------\n",
    "df_ground_truth = pd.read_csv(DATA_DIR / \"ground-truth-data.csv\")\n",
    "ground_truth    = df_ground_truth.to_dict(orient='records')\n",
    "\n",
    "print(f\"Docs: {len(documents)}, GT pairs: {len(ground_truth)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f6938f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_llm</th>\n",
       "      <th>answer_orig</th>\n",
       "      <th>document</th>\n",
       "      <th>question</th>\n",
       "      <th>course</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You can sign up for the course by visiting the...</td>\n",
       "      <td>Machine Learning Zoomcamp FAQ\\nThe purpose of ...</td>\n",
       "      <td>0227b872</td>\n",
       "      <td>Where can I sign up for the course?</td>\n",
       "      <td>machine-learning-zoomcamp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          answer_llm  \\\n",
       "0  You can sign up for the course by visiting the...   \n",
       "\n",
       "                                         answer_orig  document  \\\n",
       "0  Machine Learning Zoomcamp FAQ\\nThe purpose of ...  0227b872   \n",
       "\n",
       "                              question                     course  \n",
       "0  Where can I sign up for the course?  machine-learning-zoomcamp  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_results = pd.read_csv(DATA_DIR / \"results-gpt4o-mini.csv\")\n",
    "display(df_results.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6035b01b",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f00ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8ea5edb11da43ed93238d46ba1535f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: {'hit_rate': 0.8013831856494489, 'mrr': 0.6815251062603574}\n"
     ]
    }
   ],
   "source": [
    "from minsearch import Index  \n",
    "\n",
    "boost = {'question': 1.5, 'section': 0.1}\n",
    "\n",
    "index = Index(\n",
    "    text_fields=['question', 'section', 'text'],   \n",
    "    keyword_fields=['course']                      \n",
    ")\n",
    "index.fit(documents)\n",
    "\n",
    "def search_q1(q, k=5):\n",
    "    return index.search(\n",
    "        q['question'],\n",
    "        boost_dict=boost,    \n",
    "        num_results=k\n",
    "    )\n",
    "\n",
    "print(evaluate(ground_truth, search_q1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d9f6e7",
   "metadata": {},
   "source": [
    "* {'hit_rate': 0.8013831856494489, 'mrr': 0.6815251062603574}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad62aee",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dab35006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4550e7ea990143e5bb739780ea913660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q2: {'hit_rate': 0.3939917873352064, 'mrr': 0.2898890569843674}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from minsearch import VectorSearch\n",
    "\n",
    "texts_q = [d[\"question\"] for d in documents]\n",
    "\n",
    "pipe_q = make_pipeline(\n",
    "    TfidfVectorizer(min_df=3),\n",
    "    TruncatedSVD(n_components=128, random_state=1)\n",
    ")\n",
    "X_q = pipe_q.fit_transform(texts_q)\n",
    "\n",
    "vindex_q = VectorSearch(keyword_fields={\"course\"})\n",
    "vindex_q.fit(X_q, documents)\n",
    "\n",
    "def search_q2(q, k=5):\n",
    "    vec = pipe_q.transform([q[\"question\"]])\n",
    "    return vindex_q.search(vec, num_results=k)\n",
    "\n",
    "print(\"Q2:\", evaluate(ground_truth, search_q2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c7301e",
   "metadata": {},
   "source": [
    "* Q2: {'hit_rate': 0.3939917873352064, 'mrr': 0.2898890569843674}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464e676a",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4832e4ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d433d3e7cb041a58c2112399269be8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q3: {'hit_rate': 0.7704776312945754, 'mrr': 0.6150097255240982}\n"
     ]
    }
   ],
   "source": [
    "texts_qa = [d[\"question\"] + \" \" + d[\"text\"] for d in documents]\n",
    "\n",
    "pipe_qa = make_pipeline(\n",
    "    TfidfVectorizer(min_df=3),\n",
    "    TruncatedSVD(n_components=128, random_state=1)\n",
    ")\n",
    "X_qa = pipe_qa.fit_transform(texts_qa)\n",
    "\n",
    "vindex_qa = VectorSearch(keyword_fields={\"course\"})\n",
    "vindex_qa.fit(X_qa, documents)\n",
    "\n",
    "def search_q3(q, k=5):\n",
    "    vec = pipe_qa.transform([q[\"question\"]])\n",
    "    return vindex_qa.search(vec, num_results=k)\n",
    "\n",
    "print(\"Q3:\", evaluate(ground_truth, search_q3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba472716",
   "metadata": {},
   "source": [
    "* Q3: {'hit_rate': 0.7704776312945754, 'mrr': 0.6150097255240982}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52079156",
   "metadata": {},
   "source": [
    "# Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02050f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at jinaai/jina-embeddings-v2-small-en and are newly initialized: ['embeddings.position_embeddings.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d42db54929c9402ea7e07e44ae53ecfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28924/2258637563.py:18: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c18866c059f6432a8b38aec5f9d6db43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28924/2258637563.py:40: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  hits = client.search(collection_name=collection, query_vector=vec, limit=k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q4: {'hit_rate': 0.12102874432677761, 'mrr': 0.07618327209855186}\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient, models\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "model_name = \"jinaai/jina-embeddings-v2-small-en\"\n",
    "embedder   = SentenceTransformer(model_name)\n",
    "\n",
    "texts_q4 = [d[\"question\"] + \" \" + d[\"text\"] for d in documents]\n",
    "emb_q4   = embedder.encode(texts_q4, batch_size=64, show_progress_bar=True)\n",
    "\n",
    "client = QdrantClient(\":memory:\")\n",
    "dim = emb_q4.shape[1]\n",
    "\n",
    "collection = \"faq_vecs\"\n",
    "client.recreate_collection(\n",
    "    collection,\n",
    "    vectors_config=models.VectorParams(size=dim, distance=models.Distance.COSINE)\n",
    ")\n",
    "\n",
    "payloads = [{**doc} for doc in documents]        \n",
    "for p in payloads:\n",
    "    p[\"orig_id\"] = p[\"id\"]                       \n",
    "\n",
    "client.upload_collection(\n",
    "    collection_name=collection,\n",
    "    vectors=emb_q4,\n",
    "    payload=payloads,           \n",
    "    batch_size=256,\n",
    "    parallel=4,\n",
    ")\n",
    "\n",
    "def search_q4(q, k=5):\n",
    "    vec = embedder.encode([q[\"question\"]])[0]\n",
    "    hits = client.search(collection_name=collection, query_vector=vec, limit=k)\n",
    "    return [{\"id\": h.payload[\"orig_id\"], **h.payload} for h in hits]\n",
    "\n",
    "print(\"Q4:\", evaluate(ground_truth, search_q4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd52660",
   "metadata": {},
   "source": [
    "* Q4: {'hit_rate': 0.12102874432677761, 'mrr': 0.07618327209855186}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8e2b92",
   "metadata": {},
   "source": [
    "# Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c527a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q5 – Average cosine: 0.8415841233490403\n"
     ]
    }
   ],
   "source": [
    "pipe_cos = make_pipeline(\n",
    "    TfidfVectorizer(min_df=3),\n",
    "    TruncatedSVD(n_components=128, random_state=1)\n",
    ")\n",
    "pipe_cos.fit(\n",
    "    df_results.answer_llm + \" \" +\n",
    "    df_results.answer_orig + \" \" +\n",
    "    df_results.question\n",
    ")\n",
    "\n",
    "def cosine(u, v):\n",
    "    u_norm = np.sqrt(u.dot(u));  v_norm = np.sqrt(v.dot(v))\n",
    "    return u.dot(v) / (u_norm * v_norm)\n",
    "\n",
    "sims = []\n",
    "for _, row in df_results.iterrows():\n",
    "    v_llm  = pipe_cos.transform([row.answer_llm])[0]\n",
    "    v_orig = pipe_cos.transform([row.answer_orig])[0]\n",
    "    sims.append(cosine(v_llm, v_orig))\n",
    "\n",
    "print(\"Q5 – Average cosine:\", np.mean(sims))   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f3b4f0",
   "metadata": {},
   "source": [
    "* Q5 – Average cosine: 0.8415841233490403\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e31240b",
   "metadata": {},
   "source": [
    "# Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bebd55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q6 – Average ROUGE-1 F1: 0.3516946452113943\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "rouge = Rouge()\n",
    "\n",
    "def rouge1_f1(ref, hyp):\n",
    "    return rouge.get_scores(hyp, ref)[0][\"rouge-1\"][\"f\"]\n",
    "\n",
    "scores = [rouge1_f1(r.answer_orig, r.answer_llm) for _, r in df_results.iterrows()]\n",
    "print(\"Q6 – Average ROUGE-1 F1:\", np.mean(scores)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c21d704",
   "metadata": {},
   "source": [
    "* Q6 – Average ROUGE-1 F1: 0.3516946452113943"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
