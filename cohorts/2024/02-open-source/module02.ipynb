{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: Introduction for [LLM Zoomcamp 2024](https://courses.datatalks.club/llm-zoomcamp-2024/)\n",
    "- [homework](https://github.com/DataTalksClub/llm-zoomcamp/blob/main/cohorts/2024/02-open-source/homework.md)\n",
    "- Due date: 9 July 2024 01:00 (local time)\n",
    "- [Submit here](https://courses.datatalks.club/llm-zoomcamp-2024/homework/hw2)\n",
    "    - use private github account for log in\n",
    "\n",
    "## Extra questions\n",
    "\n",
    "- Homework URL: https://github.com/alexkolo/llm-zoomcamp-2024/blob/main/cohorts/2024/02-open-source/module02.ipynb\n",
    "- Time spent on lectures (hours): ~2h\n",
    "- Time spent on homework (hours): ~2h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Running Ollama with Docker\n",
    "\n",
    "Let's run ollama with Docker. We will need to execute the \n",
    "same command as in the lectures:\n",
    "\n",
    "```bash\n",
    "docker run -it \\\n",
    "    --rm \\\n",
    "    -v ollama:/root/.ollama \\\n",
    "    -p 11434:11434 \\\n",
    "    --name ollama \\\n",
    "    ollama/ollama\n",
    "```\n",
    "\n",
    "What's the version of ollama client? \n",
    "\n",
    "To find out, enter the container and execute `ollama` with the `-v` flag.\n",
    "\n",
    "### Docker Command explained\n",
    "\n",
    "1. **`docker run`**: This is the basic Docker command to run a container.\n",
    "\n",
    "2. **`-it`**: These are two flags combined:\n",
    "    - `-i` (interactive): Keeps the STDIN open even if not attached.\n",
    "    - `-t` (tty): Allocates a pseudo-TTY, which provides an interactive terminal session.\n",
    "\n",
    "3. **`--rm`**: Automatically removes the container when it exits. This ensures that you don't have leftover stopped containers.\n",
    "\n",
    "4. **`-v ollama:/root/.ollama`**: This mounts a volume. The `-v` flag specifies a volume mount:\n",
    "    - `ollama` is the name of the Docker volume on the host.\n",
    "    - `/root/.ollama` is the path inside the container where the volume will be mounted. This allows for persistent storage of data that is kept even when the container is removed.\n",
    "\n",
    "5. **`-p 11434:11434`**: This publishes a container's port(s) to the host. The `-p` flag specifies port mapping:\n",
    "    - `11434:11434` means that port 11434 on the host is mapped to port 11434 on the container. This makes the container's service accessible via port 11434 on the host machine.\n",
    "\n",
    "6. **`--name ollama`**: Assigns a name to the container. In this case, the container will be named \"ollama\".\n",
    "\n",
    "7. **`ollama/ollama`**: This is the image to run. `ollama/ollama` refers to a Docker image, which includes the application and its dependencies.\n",
    "\n",
    "### Code\n",
    "\n",
    "```bash\n",
    "docker exec -it ollama /bin/bash\n",
    "ollama -v\n",
    "```\n",
    "\n",
    "### Answer\n",
    "\n",
    "`0.1.48`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Downloading an LLM \n",
    "\n",
    "We will donwload a smaller LLM - gemma:2b. \n",
    "\n",
    "Again let's enter the container and pull the model:\n",
    "\n",
    "```bash\n",
    "ollama pull gemma:2b\n",
    "```\n",
    "\n",
    "In docker, it saved the results into `/root/.ollama`\n",
    "\n",
    "We're interested in the metadata about this model. You can find\n",
    "it in `models/manifests/registry.ollama.ai/library`\n",
    "\n",
    "What's the content of the file related to gemma?\n",
    "\n",
    "\n",
    "### Code\n",
    "\n",
    "```bash\n",
    "docker exec -it ollama /bin/bash\n",
    "ollama pull gemma:2b\n",
    "cat /root/.ollama/models/manifests/registry.ollama.ai/library/gemma/2b\n",
    "```\n",
    "\n",
    "### Answer\n",
    "\n",
    "```json\n",
    "{\"schemaVersion\":2,\"mediaType\":\"application/vnd.docker.distribution.manifest.v2+json\",\"config\":{\"mediaType\":\"application/vnd.docker.container.image.v1+json\",\"digest\":\"sha256:887433b89a901c156f7e6944442f3c9e57f3c55d6ed52042cbb7303aea994290\",\"size\":483},\"layers\":[{\"mediaType\":\"application/vnd.ollama.image.model\",\"digest\":\"sha256:c1864a5eb19305c40519da12cc543519e48a0697ecd30e15d5ac228644957d12\",\"size\":1678447520},{\"mediaType\":\"application/vnd.ollama.image.license\",\"digest\":\"sha256:097a36493f718248845233af1d3fefe7a303f864fae13bc31a3a9704229378ca\",\"size\":8433},{\"mediaType\":\"application/vnd.ollama.image.template\",\"digest\":\"sha256:109037bec39c0becc8221222ae23557559bc594290945a2c4221ab4f303b8871\",\"size\":136},{\"mediaType\":\"application/vnd.ollama.image.params\",\"digest\":\"sha256:22a838ceb7fb22755a3b0ae9b4eadde629d19be1f651f73efb8c6b4e2cd0eea0\",\"size\":84}]}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Running the LLM\n",
    "\n",
    "Test the following prompt: \"10 * 10\". What's the answer?\n",
    "\n",
    "### Code\n",
    "\n",
    "```bash\n",
    "docker exec -it ollama /bin/bash\n",
    "ollama pull gemma:2b\n",
    "ollama run gemma:2b \"10 * 10\"   \n",
    "```\n",
    "\n",
    "### Answer\n",
    "\n",
    "```html\n",
    "Sure, here is the answer:\n",
    "\n",
    "10 * 10<sup>end_of_turn</sup>\n",
    "\n",
    "This expression evaluates to 100, which is 10 multiplied by 10<sup>2</sup>.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. Donwloading the weights \n",
    "\n",
    "We don't want to pull the weights every time we run\n",
    "a docker container. Let's do it once and have them available\n",
    "every time we start a container.\n",
    "\n",
    "First, we will need to change how we run the container.\n",
    "\n",
    "Instead of mapping the `/root/.ollama` folder to a named volume,\n",
    "let's map it to a local directory:\n",
    "\n",
    "```bash\n",
    "mkdir ollama_files\n",
    "\n",
    "docker run -it \\\n",
    "    --rm \\\n",
    "    -v ./ollama_files:/root/.ollama \\\n",
    "    -p 11434:11434 \\\n",
    "    --name ollama \\\n",
    "    ollama/ollama\n",
    "```\n",
    "\n",
    "Now pull the model:\n",
    "\n",
    "```bash\n",
    "docker exec -it ollama ollama pull gemma:2b \n",
    "```\n",
    "\n",
    "What's the size of the `ollama_files/models` folder? \n",
    "\n",
    "* 0.6G\n",
    "* 1.2G\n",
    "* 1.7G\n",
    "* 2.2G\n",
    "\n",
    "Hint: on linux, you can use `du -h` for that.\n",
    "\n",
    "### Code\n",
    "\n",
    "1. Terminal\n",
    "  \n",
    "    ```bash\n",
    "    mkdir ollama_files\n",
    "\n",
    "    docker run -it \\\n",
    "        --rm \\\n",
    "        -v ./ollama_files:/root/.ollama \\\n",
    "        -p 11434:11434 \\\n",
    "        --name ollama \\\n",
    "        ollama/ollama\n",
    "    ```\n",
    "\n",
    "2. Terminal\n",
    "\n",
    "    ```bash\n",
    "    docker exec -it ollama ollama pull gemma:2b \n",
    "    du -h ollama_files\n",
    "    ```\n",
    "\n",
    "### Answer\n",
    "\n",
    "`1.6G    ollama_files/models`\n",
    "\n",
    "-> closest: ` 1.7G`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean\n",
    "\n",
    "```bash\n",
    "docker exec -it ollama /bin/bash\n",
    "ollama list\n",
    "ollama rm gemma:2b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. Adding the weights \n",
    "\n",
    "Let's now stop the container and add the weights \n",
    "to a new image\n",
    "\n",
    "For that, let's create a `Dockerfile`:\n",
    "\n",
    "```dockerfile\n",
    "FROM ollama/ollama\n",
    "\n",
    "COPY ...\n",
    "```\n",
    "\n",
    "What do you put after `COPY`?\n",
    "\n",
    "### Code\n",
    "\n",
    "```bash\n",
    "docker build -f ./cohorts/2024/02-open-source/Dockerfile -t new_ollama_image .\n",
    "docker run -it --rm -p 11434:11434 --name new_ollama_container new_ollama_image\n",
    "# another terminal\n",
    "docker exec -it new_ollama_container /bin/bash\n",
    "```\n",
    "\n",
    "```bash\n",
    "docker stop new_ollama_container\n",
    "docker rmi new_ollama_image\n",
    "docker images # check if removed\n",
    "```\n",
    "\n",
    "### Answer\n",
    "\n",
    "`ollama_files /root/.ollama`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6. Serving it \n",
    "\n",
    "Let's build it:\n",
    "\n",
    "```bash\n",
    "docker build -t ollama-gemma2b .\n",
    "```\n",
    "\n",
    "And run it:\n",
    "\n",
    "```bash\n",
    "docker run -it --rm -p 11434:11434 ollama-gemma2b\n",
    "```\n",
    "\n",
    "We can connect to it using the OpenAI client\n",
    "\n",
    "Let's test it with the following prompt:\n",
    "\n",
    "```python\n",
    "prompt = \"What's the formula for energy?\"\n",
    "```\n",
    "\n",
    "Also, to make results reproducible, set the `temperature` parameter to 0:\n",
    "\n",
    "```bash\n",
    "response = client.chat.completions.create(\n",
    "    #...\n",
    "    temperature=0.0\n",
    ")\n",
    "```\n",
    "\n",
    "How many completion tokens did you get in response?\n",
    "\n",
    "* 304\n",
    "* 604\n",
    "* 904\n",
    "* 1204\n",
    "\n",
    "### Answer\n",
    "\n",
    "- for `tiktoken` \"gpt-4o\": 256\n",
    "- for `tiktoken` \"gpt-4\": 261\n",
    "- https://tiktokenizer.vercel.app/  gemma-7b : 292\n",
    "\n",
    "\n",
    "-> closest: `304`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code\n",
    "\n",
    "```bash\n",
    "docker build -f ./cohorts/2024/02-open-source/Dockerfile -t ollama-gemma2b .\n",
    "docker run -it --rm -p 11434:11434 ollama-gemma2b\n",
    "```\n",
    "\n",
    "# another terminal\n",
    "`docker exec -it new_ollama_container /bin/bash`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1/\",\n",
    "    api_key=\"ollama\",\n",
    ")\n",
    "prompt = \"What's the formula for energy?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gemma:2b\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "repoonse_text = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the text to get the tokens\n",
    "tokens = encoding.encode(repoonse_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 6. Number of tokens 261\n"
     ]
    }
   ],
   "source": [
    "print(f\"Question 6. Number of tokens {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's the formula for energy:\n",
      "\n",
      "**E = K + U**\n",
      "\n",
      "Where:\n",
      "\n",
      "* **E** is the energy in joules (J)\n",
      "* **K** is the kinetic energy in joules (J)\n",
      "* **U** is the potential energy in joules (J)\n",
      "\n",
      "**Kinetic energy (K)** is the energy an object possesses when it moves or is in motion. It is calculated as half the product of an object's mass (m) and its velocity (v) squared:\n",
      "\n",
      "**K = 1/2mv^2**\n",
      "\n",
      "**Potential energy (U)** is the energy an object possesses due to its position or configuration. It is calculated as the product of an object's mass, gravitational constant (g), and height or position above a reference point.\n",
      "\n",
      "**U = mgh**\n",
      "\n",
      "Where:\n",
      "\n",
      "* **m** is the mass in kilograms (kg)\n",
      "* **g** is the gravitational constant (9.8 m/s^2)\n",
      "* **h** is the height or position in meters (m)\n",
      "\n",
      "The formula shows that energy can be expressed as the sum of kinetic and potential energy. The kinetic energy is a measure of the object's ability to do work, while the potential energy is a measure of the object's ability to do work against a force.\n"
     ]
    }
   ],
   "source": [
    "print(repoonse_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean docker\n",
    "\n",
    "`docker system prune -a`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
